{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import tqdm\n",
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read people.csv...\n",
      "Load act_train.csv...\n",
      "Load act_test.csv...\n",
      "Process tables...\n"
     ]
    }
   ],
   "source": [
    "print(\"Read people.csv...\")\n",
    "people=pd.read_csv('people.csv',\n",
    "                   dtype={'char_38': np.int32},\n",
    "                   parse_dates=['date'])\n",
    "\n",
    "print(\"Load act_train.csv...\")\n",
    "act_train=pd.read_csv('act_train.csv',\n",
    "                      parse_dates=['date'])\n",
    "\n",
    "print(\"Load act_test.csv...\")\n",
    "act_test=pd.read_csv('act_test.csv',\n",
    "                     parse_dates=['date'])\n",
    "\n",
    "\n",
    "print(\"Process tables...\")\n",
    "for table in [act_train, act_test]:\n",
    "    table['year'] = table['date'].dt.year\n",
    "    table['month'] = table['date'].dt.month\n",
    "    table['day'] = table['date'].dt.day\n",
    "    table['weekday'] = table['date'].dt.weekday\n",
    "    ##table.drop('date', axis=1, inplace=True)\n",
    "    table['activity_category'] = table['activity_category'].str.lstrip('type ').astype(np.int32)\n",
    "    for i in range(1, 11):\n",
    "        table['char_' + str(i)].fillna('type -999', inplace=True)\n",
    "        table['char_' + str(i)] = table['char_' + str(i)].str.lstrip('type ').astype(np.int32)\n",
    "    table['people_id'] = table['people_id'].str.lstrip('ppl_').astype(np.float).astype(np.int32)\n",
    "\n",
    "people['year'] = people['date'].dt.year\n",
    "people['month'] = people['date'].dt.month\n",
    "people['day'] = people['date'].dt.day\n",
    "people['weekday'] = people['date'].dt.weekday\n",
    "\n",
    "#people.drop('date', axis=1, inplace=True)\n",
    "people['group_1'] = people['group_1'].str.lstrip('group ').astype(np.int32)\n",
    "people['people_id'] = people['people_id'].str.lstrip('ppl_').astype(np.float).astype(np.int32)\n",
    "for i in range(1, 10):\n",
    "    people['char_' + str(i)] = people['char_' + str(i)].str.lstrip('type ').astype(np.int32)\n",
    "for i in range(10, 38):\n",
    "    people['char_' + str(i)] = people['char_' + str(i)].astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people_id: 189118\n",
      "char_1: 2\n",
      "group_1: 34224\n",
      "char_2: 3\n",
      "date: 1196\n",
      "char_3: 43\n",
      "char_4: 25\n",
      "char_5: 9\n",
      "char_6: 7\n",
      "char_7: 25\n",
      "char_8: 8\n",
      "char_9: 9\n",
      "char_10: 2\n",
      "char_11: 2\n",
      "char_12: 2\n",
      "char_13: 2\n",
      "char_14: 2\n",
      "char_15: 2\n",
      "char_16: 2\n",
      "char_17: 2\n",
      "char_18: 2\n",
      "char_19: 2\n",
      "char_20: 2\n",
      "char_21: 2\n",
      "char_22: 2\n",
      "char_23: 2\n",
      "char_24: 2\n",
      "char_25: 2\n",
      "char_26: 2\n",
      "char_27: 2\n",
      "char_28: 2\n",
      "char_29: 2\n",
      "char_30: 2\n",
      "char_31: 2\n",
      "char_32: 2\n",
      "char_33: 2\n",
      "char_34: 2\n",
      "char_35: 2\n",
      "char_36: 2\n",
      "char_37: 2\n",
      "char_38: 101\n",
      "year: 4\n",
      "month: 12\n",
      "day: 31\n",
      "weekday: 7\n",
      "\n",
      "people_id: 151295\n",
      "activity_id: 2197291\n",
      "date: 411\n",
      "activity_category: 7\n",
      "char_1: 52\n",
      "char_2: 33\n",
      "char_3: 12\n",
      "char_4: 8\n",
      "char_5: 8\n",
      "char_6: 6\n",
      "char_7: 9\n",
      "char_8: 19\n",
      "char_9: 20\n",
      "char_10: 6516\n",
      "outcome: 2\n",
      "year: 2\n",
      "month: 12\n",
      "day: 31\n",
      "weekday: 7\n",
      "\n",
      "people_id: 37823\n",
      "activity_id: 498687\n",
      "date: 411\n",
      "activity_category: 7\n",
      "char_1: 49\n",
      "char_2: 32\n",
      "char_3: 12\n",
      "char_4: 8\n",
      "char_5: 7\n",
      "char_6: 6\n",
      "char_7: 9\n",
      "char_8: 19\n",
      "char_9: 20\n",
      "char_10: 3962\n",
      "year: 2\n",
      "month: 12\n",
      "day: 31\n",
      "weekday: 7\n"
     ]
    }
   ],
   "source": [
    "for name in people.columns.values:\n",
    "    print('%s: %d' % (name, len(people[name].unique())))\n",
    "print('')    \n",
    "for name in act_train.columns.values:\n",
    "    print('%s: %d' % (name, len(act_train[name].unique())))\n",
    "print('')    \n",
    "for name in act_test.columns.values:\n",
    "    print('%s: %d' % (name, len(act_test[name].unique())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train=pd.merge(people, act_train, on='people_id')\n",
    "test=pd.merge(people, act_test, on='people_id')\n",
    "del(act_train)\n",
    "del(act_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mutual_entropy(a, l):\n",
    "    n = len(a)\n",
    "    a_ent = -np.sum([x*np.log(x) for x in a.value_counts()/n])\n",
    "    l_ent = -np.sum([x*np.log(x) for x in l.value_counts()/n])\n",
    "    al_ent = -np.sum([x*np.log(x) for x in a[l==0].value_counts()/n]) - \\\n",
    "             np.sum([x*np.log(x) for x in a[l==1].value_counts()/n])\n",
    "    return a_ent + l_ent - al_ent\n",
    "\n",
    "def iteraction_entropy(a, b, l):\n",
    "    n = len(a)\n",
    "    aa=np.array(a)\n",
    "    bb=np.array(b)\n",
    "    ll=np.array(l)\n",
    "    ab = pd.Series([(aa[i], bb[i]) for i in np.arange(0, n)])\n",
    "    abl_ent=-np.sum([x*np.log(x) for x in ab[ll==0].value_counts()/n]) - \\\n",
    "            np.sum([x*np.log(x) for x in ab[ll==1].value_counts()/n])\n",
    "    ab_ent =-np.sum([x*np.log(x) for x in ab.value_counts()/ n])\n",
    "    l_ent = -np.sum([x*np.log(x) for x in l.value_counts()/n])\n",
    "    a_ent = -np.sum([x*np.log(x) for x in a.value_counts()/n])\n",
    "    b_ent = -np.sum([x*np.log(x) for x in b.value_counts()/n])\n",
    "    abl_ment = ab_ent + l_ent - abl_ent\n",
    "    return abl_ment - mutual_entropy(a, l) - mutual_entropy(b, l) , ab_ent - a_ent - b_ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def get_log_posterior(train, name):\n",
    "    s0=train[train['outcome'] == 0][name]\n",
    "    s0=s0.append(pd.DataFrame({0:train[name].unique()}))[0]\n",
    "    s0=-np.log(s0.value_counts(dropna=False)/len(s0))\n",
    "    s0=dict(s0)\n",
    "    s1=train[train['outcome'] == 1][name]    \n",
    "    s1=s1.append(pd.DataFrame({0:train[name].unique()}))[0]\n",
    "    s1=-np.log(s1.value_counts(dropna=False)/len(s1))\n",
    "    s1=dict(s1)\n",
    "    return s0,s1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_1_x: 0.0128333413646\n",
      "group_1: 0.623434548023\n",
      "char_2_x: 0.29692885141\n",
      "date_x: 0.0947567469661\n",
      "char_3_x: 0.0248090040523\n",
      "char_4_x: 0.0137991158875\n",
      "char_5_x: 0.0137259575534\n",
      "char_6_x: 0.0449202668956\n",
      "char_7_x: 0.075758721909\n",
      "char_8_x: 0.091276730308\n",
      "char_9_x: 0.0818365865575\n",
      "char_10_x: 0.0363928982184\n",
      "char_11: 0.0256503268109\n",
      "char_12: 0.0253169791715\n",
      "char_13: 0.052352334771\n",
      "char_14: 0.0307289452823\n",
      "char_15: 0.0349814003224\n",
      "char_16: 0.03903751061\n",
      "char_17: 0.0414109644556\n",
      "char_18: 0.022403188299\n",
      "char_19: 0.0392695008341\n",
      "char_20: 0.0369277498289\n",
      "char_21: 0.0390688017897\n",
      "char_22: 0.0411577565609\n",
      "char_23: 0.0369545764241\n",
      "char_24: 0.0245830476445\n",
      "char_25: 0.0419558187753\n",
      "char_26: 0.0148824036234\n",
      "char_27: 0.0276123033534\n",
      "char_28: 0.0387835425398\n",
      "char_29: 0.0176502836649\n",
      "char_30: 0.022189949835\n",
      "char_31: 0.0322979528096\n",
      "char_32: 0.0391130346572\n",
      "char_33: 0.0227434824364\n",
      "char_34: 0.049567378195\n",
      "char_35: 0.023148879453\n",
      "char_36: 0.0504762355423\n",
      "char_37: 0.0421365894057\n",
      "year_x: 0.00159423144969\n",
      "month_x: 0.0120170033941\n",
      "day_x: 0.0173870100065\n",
      "weekday_x: 0.010171959524\n",
      "date_y: 0.0323109017496\n",
      "activity_category: 0.0203031266251\n",
      "char_1_y: 0.000797163274473\n",
      "char_2_y: 0.00075572142338\n",
      "char_3_y: 0.00034918342595\n",
      "char_4_y: 0.000419867520503\n",
      "char_5_y: 0.000653375258075\n",
      "char_6_y: 0.000327232550035\n",
      "char_7_y: 0.00025092021974\n",
      "char_8_y: 0.000565997644437\n",
      "char_9_y: 0.000600463625506\n",
      "char_10_y: 0.0855559074246\n",
      "outcome: 0.686851739242\n",
      "year_y: 0.00226341855023\n",
      "month_y: 0.00709262300129\n",
      "day_y: 0.00588977618362\n",
      "weekday_y: 0.0012193050691\n"
     ]
    }
   ],
   "source": [
    "a=dict()\n",
    "b=dict()\n",
    "gain=dict()\n",
    "for name in train.columns.values:\n",
    "    if name not in set(['people_id', 'activity_id', 'char_38']):\n",
    "        print(name, end=': ')\n",
    "        gain[name] = mutual_entropy(train[name], train['outcome'])\n",
    "        print(gain[name])\n",
    "        a[name], b[name] = get_log_posterior(train, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_log_posterior2(train, name1, name2):\n",
    "    n=len(train)\n",
    "    aa=np.array(train[name1])\n",
    "    bb=np.array(train[name2])\n",
    "    ab=pd.Series([(aa[i], bb[i]) for i in np.arange(0, n)])\n",
    "    s0=ab[np.array(train['outcome'] == 0)]\n",
    "    s0=s0.append(pd.DataFrame({0:ab.unique()}))[0]\n",
    "    s0=-np.log(s0.value_counts(dropna=False)/len(s0))\n",
    "    s0=dict(s0)\n",
    "    s1=ab[np.array(train['outcome'] == 1)]\n",
    "    s1=s1.append(pd.DataFrame({0:ab.unique()}))[0]\n",
    "    s1=-np.log(s1.value_counts(dropna=False)/len(s1))\n",
    "    s1=dict(s1)\n",
    "    return s0,s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "field_pairs=set()\n",
    "for name, name2 in itertools.combinations(train.columns.values, 2):\n",
    "    non_categorical_fields=set(['people_id', 'activity_id', 'char_38'])\n",
    "    if name not in non_categorical_fields and name2 not in non_categorical_fields:\n",
    "        ie, lb = iteraction_entropy(train[name], train[name2], train['outcome'])\n",
    "        if ie > 0.01:\n",
    "            print(name, name2, end=': ')\n",
    "            print(ie)\n",
    "            field_pairs.add((name, name2))\n",
    "            a[(name, name2)], b[(name, name2)] = get_log_posterior2(train, name, name2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_log_posterior_by_key(s, name, x):\n",
    "    try:\n",
    "        return s[name][x]\n",
    "    except KeyError:\n",
    "        return -999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aa=np.zeros(len(test))\n",
    "bb=np.zeros(len(test))\n",
    "for name in test.columns.values:\n",
    "    if name not in set(['people_id', 'activity_id', 'char_38']):\n",
    "        aa = aa + np.array([get_log_posterior_by_key(a, name, x) for x in test[name]])\n",
    "        bb = bb + np.array([get_log_posterior_by_key(b, name, x) for x in test[name]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission=pd.DataFrame({'activity_id':act_test['activity_id'], 'outcome':np.argmin(np.concatenate((aa, bb)).reshape((2, len(aa))).T, 1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aa, bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1733318, 1745884, 1796268, 1710044, 1803650]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "folds_people=pd.DataFrame({'fold':np.random.randint(0, 5, len(people['people_id'].unique())), 'people_id': people['people_id'].unique()})\n",
    "#folds=np.zeros(len(train))\n",
    "cv=pd.merge(folds_people, train, on='people_id')\n",
    "print([sum(cv['fold']!=0),sum(cv['fold']!=1),sum(cv['fold']!=2),sum(cv['fold']!=3),sum(cv['fold']!=4)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1935284, 1943126, 1867439, 1906720, 1136595]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "folds_group_1=pd.DataFrame({'fold':np.random.randint(0, 5, len(people['group_1'].unique())), 'group_1': people['group_1'].unique()})\n",
    "cv2=pd.merge(folds_group_1, train, on='group_1')\n",
    "print([sum(cv2['fold']!=0),sum(cv2['fold']!=1),sum(cv2['fold']!=2),sum(cv2['fold']!=3),sum(cv2['fold']!=4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create (sum(outcome), count) on the grid of group_1 x date\n",
    "def create_outcome_group(train, namelist):\n",
    "    outcome_group=train.groupby(namelist+['people_id'])['outcome']\n",
    "    outcome_group=pd.DataFrame(outcome_group.mean()).reset_index()\n",
    "    outcome_group=outcome_group.groupby(namelist)['outcome']\n",
    "    return outcome_group\n",
    "\n",
    "def create_group_pair(outcome_group):\n",
    "    group_pair_1=dict(outcome_group.sum())\n",
    "    group_pair_2=dict(outcome_group.count())\n",
    "    group_pair = {key:(group_pair_1[key], group_pair_2[key]) for key in group_pair_2.keys()}    \n",
    "    return group_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating counting features for train ... \n",
      "char_1_x,group_1,char_2_x,char_3_x,char_4_x,char_5_x,char_6_x,char_7_x,char_8_x,char_9_x,char_10_x,char_11,char_12,char_13,char_14,char_15,char_16,char_17,char_18,char_19,char_20,char_21,char_22,char_23,char_24,char_25,char_26,char_27,char_28,char_29,char_30,char_31,char_32,char_33,char_34,char_35,char_36,char_37,year_x,month_x,day_x,weekday_x,activity_category,char_1_y,char_2_y,char_3_y,char_4_y,char_5_y,char_6_y,char_7_y,char_8_y,char_9_y,char_10_y,year_y,month_y,day_y,weekday_y,Creating counting features for all ... \n",
      "activity_category,char_10_x,char_10_y,char_11,char_12,char_13,char_14,char_15,char_16,char_17,char_18,char_19,char_1_x,char_1_y,char_20,char_21,char_22,char_23,char_24,char_25,char_26,char_27,char_28,char_29,char_2_x,char_2_y,char_30,char_31,char_32,char_33,char_34,char_35,char_36,char_37,char_3_x,char_3_y,char_4_x,char_4_y,char_5_x,char_5_y,char_6_x,char_6_y,char_7_x,char_7_y,char_8_x,char_8_y,char_9_x,char_9_y,day_x,day_y,group_1,month_x,month_y,weekday_x,weekday_y,year_x,year_y,"
     ]
    }
   ],
   "source": [
    "# create leave-one-person-out counting/proportion features \n",
    "\n",
    "def create_counting_features(thistrain):\n",
    "    c=dict()\n",
    "    people_list=thistrain['people_id'].unique()\n",
    "    people_group=thistrain.groupby('people_id')\n",
    "    for name in thistrain.columns.values:\n",
    "        if name not in set(['people_id', 'activity_id', 'fold', 'char_38', 'outcome', 'date_x', 'date_y']) and \\\n",
    "            not name.startswith('extrapolate'):\n",
    "                print(name, end=',')\n",
    "                c_total=thistrain[name].value_counts(dropna=False)\n",
    "                count_total=sum(c_total)\n",
    "                c_people=people_group[name].value_counts(dropna=False)\n",
    "                c_people=c_people.reset_index(name='counts')\n",
    "                people_count_total=c_people.groupby('people_id')['counts'].sum()\n",
    "                c_people['proportion']=c_people.apply(lambda x: (c_total[x[name]] - x['counts']) / (count_total - people_count_total[x['people_id']]), axis=1)\n",
    "                c[name]=c_people.groupby(['people_id', name])['proportion'].mean()\n",
    "    print('... [Done]')\n",
    "    return c\n",
    "\n",
    "print('Creating counting features for train ... ')\n",
    "c=create_counting_features(cv)\n",
    "print('Creating counting features for all ... ')\n",
    "c_all=create_counting_features(pd.concat((train, test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "def create_od_group_pair(group_pair):\n",
    "    a=collections.OrderedDict(sorted(group_pair.items()))\n",
    "#    print(list(a.items())[0:20])\n",
    "    return list(a.keys()), list(a.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "def get_outcome_mean_leaveoneout(group_pair, key_pair, value):\n",
    "    try:\n",
    "        a=group_pair[key_pair]\n",
    "        if a[1]>1:\n",
    "            return (a[0]-value)/(a[1]-1)\n",
    "        else:\n",
    "            return -999\n",
    "    except:\n",
    "        return -999\n",
    "    \n",
    "def get_outcome_mean_include(group_pair, key_pair):\n",
    "    try:\n",
    "        a=group_pair[key_pair]\n",
    "        return a[0]/a[1]\n",
    "    except:\n",
    "        return -999\n",
    "    \n",
    "def get_outcome_mean(group_pair, row, namelist):\n",
    "    return get_outcome_mean_include(group_pair[row['fold']], tuple(row[namelist]))\n",
    "\n",
    "def get_outcome_mean_train(group_pair, row, namelist):\n",
    "    return get_outcome_mean_leaveoneout(group_pair['train'], tuple(row[namelist]), row['outcome'])\n",
    "\n",
    "def get_outcome_mean_test(group_pair, row, namelist):\n",
    "    return get_outcome_mean_include(group_pair['train'], tuple(row[namelist]))\n",
    "\n",
    "def get_outcome_mean1(group_pair, row, name):\n",
    "    return get_outcome_mean_include(group_pair[row['fold']], row[name])\n",
    "\n",
    "def get_outcome_mean1_train(group_pair, row, name):\n",
    "    return get_outcome_mean_leaveoneout(group_pair['train'], row[name], row['outcome'])\n",
    "\n",
    "def get_outcome_mean1_test(group_pair, row, name):\n",
    "    return get_outcome_mean_include(group_pair['train'], row[name])\n",
    "\n",
    "def get_outcome_mean2(group_pair, row, name1, name2):\n",
    "    return get_outcome_mean_include(group_pair[row['fold']], (row[name1], row[name2]))\n",
    "\n",
    "def get_outcome_mean2_train(group_pair, row, name1, name2):\n",
    "    return get_outcome_mean_leaveoneout(group_pair['train'], (row[name1], row[name2]), row['outcome'])\n",
    "\n",
    "def get_outcome_mean2_test(group_pair, row, name1, name2):\n",
    "    return get_outcome_mean_include(group_pair['train'], (row[name1], row[name2]))\n",
    "\n",
    "import bisect\n",
    "def get_outcome_approxmean_leaveoneout(od_group_pair_keys, od_group_pair_items, row, namelist, datename, value):\n",
    "    try:\n",
    "        ind = bisect.bisect_left(od_group_pair_keys, tuple(row[namelist + [datename]]))\n",
    "        twoside=0.\n",
    "        count=0.\n",
    "        backward=0.\n",
    "        keys=tuple(row[namelist])\n",
    "        date=row[datename]\n",
    "        if od_group_pair_keys[ind][:-1] == keys and od_group_pair_keys[ind][-1] == date:\n",
    "            a = od_group_pair_items[ind][-1]\n",
    "            if a[1]>1:\n",
    "                return (a[0] - value) / (a[1] - 1)\n",
    "            else:\n",
    "                backward=ind+1\n",
    "        else:\n",
    "            backward=ind\n",
    "        forward=ind-1\n",
    "        if od_group_pair_keys[backward][:-1] == keys:\n",
    "            a = od_group_pair_items[backward][-1]\n",
    "            weight=1/((od_group_pair_keys[backward][-1]-date).days+1)\n",
    "            twoside = twoside + (a[0]/a[1])*weight\n",
    "            count = count + weight\n",
    "            \n",
    "        if od_group_pair_keys[forward][:-1] == keys:\n",
    "            a = od_group_pair_items[forward][-1]\n",
    "            weight=1/((date-od_group_pair_keys[forward][-1]).days+1)\n",
    "            twoside = twoside + (a[0]/a[1])*weight\n",
    "            count = count + weight\n",
    "        if count > 0:\n",
    "            return twoside/count\n",
    "        else:\n",
    "            return -999\n",
    "    except:\n",
    "        return -999\n",
    "    \n",
    "def get_outcome_approxmean_include(od_group_pair_keys, od_group_pair_items, row, namelist, datename):\n",
    "    try:\n",
    "        ind = bisect.bisect_left(od_group_pair_keys, tuple(row[namelist + [datename]]))\n",
    "        twoside=0.\n",
    "        count=0.\n",
    "        keys=tuple(row[namelist])\n",
    "        date=row[datename]\n",
    "        if od_group_pair_keys[ind][:-1] == keys:\n",
    "            a = od_group_pair_items[ind][-1]\n",
    "            weight=1/((od_group_pair_keys[ind][-1]-date).days+1)\n",
    "            twoside = twoside + (a[0]/a[1])*weight\n",
    "            count = count + weight\n",
    "        if od_group_pair_keys[ind][-1] != date and od_group_pair_keys[ind-1][:-1] == keys:\n",
    "            a = od_group_pair_items[ind-1][-1]\n",
    "            weight=1/((date-od_group_pair_keys[ind-1][-1]).days+1)\n",
    "            twoside = twoside + (a[0]/a[1])*weight\n",
    "            count = count + weight\n",
    "        if count > 0:\n",
    "            return twoside/count\n",
    "        else:\n",
    "            return -999\n",
    "    except:\n",
    "        return -999\n",
    "\n",
    "def get_outcome_approxmean(od_group_pair_keys, od_group_pair_items, row, name, datename):\n",
    "    return get_outcome_approxmean_include(od_group_pair_keys[row['fold']], od_group_pair_items[row['fold']], row, name, datename) \n",
    "\n",
    "def get_outcome_approxmean_train(od_group_pair_keys, od_group_pair_items, row, name, datename):\n",
    "    return get_outcome_approxmean_leaveoneout(od_group_pair_keys['train'], od_group_pair_items['train'], row, name, datename, row['outcome'])\n",
    "\n",
    "def get_outcome_approxmean_test(od_group_pair_keys, od_group_pair_items, row, name, datename):\n",
    "    return get_outcome_approxmean_include(od_group_pair_keys['train'], od_group_pair_items['train'], row, name, datename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import collections\n",
    "import bisect\n",
    "\n",
    "\n",
    "\n",
    "def prebuild_feature(namelist):\n",
    "    print('Process outcome group ... ')\n",
    "    outcome_group=dict()\n",
    "    for i in [0, 1, 2, 3, 4]:\n",
    "        outcome_group[i]=create_outcome_group(train[cv['fold']!=i], namelist)\n",
    "    outcome_group['train']=create_outcome_group(train, namelist)   \n",
    "    print('Process group pair ...')\n",
    "    group_pair=dict()\n",
    "    for i in [0, 1, 2, 3, 4]:\n",
    "        group_pair[i] = create_group_pair(outcome_group[i])\n",
    "    group_pair['train']=create_group_pair(outcome_group['train'])\n",
    "    print('Get Pairs ...')\n",
    "    tqdm.tqdm_pandas(tqdm.tqdm_notebook())\n",
    "    if len(namelist) > 2:\n",
    "        thiscv=cv.progress_apply(lambda x: get_outcome_mean(group_pair, x, namelist), axis=1)\n",
    "#        thistrain=train.progress_apply(lambda x: get_outcome_mean_train(group_pair, x, namelist), axis=1)\n",
    "        thistest=test.progress_apply(lambda x: get_outcome_mean_test(group_pair, x, namelist), axis=1)\n",
    "    elif len(namelist) == 2:\n",
    "        thiscv=cv.progress_apply(lambda x: get_outcome_mean2(group_pair, x, namelist[0], namelist[1]), axis=1)\n",
    "#        thistrain=train.progress_apply(lambda x: get_outcome_mean2_train(group_pair, x, namelist[0], namelist[1]), axis=1)\n",
    "        thistest=test.progress_apply(lambda x: get_outcome_mean2_test(group_pair, x, namelist[0], namelist[1]), axis=1)         \n",
    "    elif len(namelist) == 1:\n",
    "        thiscv=cv.progress_apply(lambda x: get_outcome_mean1(group_pair, x, namelist[0]), axis=1)\n",
    "#        thistrain=train.progress_apply(lambda x: get_outcome_mean1_train(group_pair, x, namelist[0]), axis=1)\n",
    "        thistest=test.progress_apply(lambda x: get_outcome_mean1_test(group_pair, x, namelist[0]), axis=1)        \n",
    "    return thiscv, thiscv, thistest\n",
    "           \n",
    "    \n",
    "def prebuild_feature_with_date(namelist, datename, renamed_feature=None):\n",
    "    print('Process outcome group ... ')\n",
    "    outcome_group=dict()\n",
    "    for i in [0, 1, 2, 3, 4]:\n",
    "        outcome_group[i]=create_outcome_group(train[cv['fold']!=i], namelist + [datename])\n",
    "    outcome_group['train']=create_outcome_group(train, namelist + [datename])\n",
    "    print('Process group pair ...')\n",
    "    group_pair=dict()\n",
    "    for i in [0, 1, 2, 3, 4]:\n",
    "        group_pair[i] = create_group_pair(outcome_group[i])\n",
    "    group_pair['train']=create_group_pair(outcome_group['train'])\n",
    "    print('Create ordered-dict group pair ...')\n",
    "    od_group_pair_keys=dict()\n",
    "    od_group_pair_items=dict()\n",
    "    for i in [0, 1, 2, 3, 4]:\n",
    "        od_group_pair_keys[i], od_group_pair_items[i]=create_od_group_pair(group_pair[i])\n",
    "    od_group_pair_keys['train'], od_group_pair_items['train']=create_od_group_pair(group_pair['train'])\n",
    "    print('Get Approximate Pairs ...')\n",
    "    tqdm.tqdm_pandas(tqdm.tqdm_notebook())\n",
    "    if renamed_feature is None:\n",
    "        thiscv=cv.progress_apply(lambda x: get_outcome_approxmean(od_group_pair_keys, od_group_pair_items, x, namelist, datename), axis=1)\n",
    "#        thistrain=train.progress_apply(lambda x: get_outcome_approxmean_train(od_group_pair_keys, od_group_pair_items, x, namelist, datename), axis=1)\n",
    "        thistest=test.progress_apply(lambda x: get_outcome_approxmean_test(od_group_pair_keys, od_group_pair_items, x, namelist, datename), axis=1)\n",
    "        return thiscv, thiscv, thistest              \n",
    "    else:\n",
    "        thiscv=cv.progress_apply(lambda x: \\\n",
    "                                 get_outcome_approxmean(od_group_pair_keys, od_group_pair_items, x, namelist, datename) \\\n",
    "                                 if x[renamed_feature]==-999 else x[renamed_feature], \\\n",
    "                                 axis=1)\n",
    "#        thistrain=train.progress_apply(lambda x: \\\n",
    "#                                    get_outcome_approxmean_train(od_group_pair_keys, od_group_pair_items, x, namelist, datename) \\\n",
    "#                                    if x[renamed_feature]==-999 else x[renamed_feature], \\\n",
    "#                                    axis=1)\n",
    "        thistest=test.progress_apply(lambda x: \\\n",
    "                                   get_outcome_approxmean_test(od_group_pair_keys, od_group_pair_items, x, namelist, datename) \\\n",
    "                                   if x[renamed_feature]==-999 else x[renamed_feature], \\\n",
    "                                   axis=1)\n",
    "        return thiscv,thiscv,thistest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cv['extrapolate'],train['extrapolate'], test['extrapolate'] = prebuild_feature(['group_1', 'date_y'])\n",
    "#cv['extrapolate'].to_csv('cv:extrapolate.csv')\n",
    "#train['extrapolate'].to_csv('train:extrapolate.csv')\n",
    "#test['extrapolate'].to_csv('test:extrapolate.csv')\n",
    "cv['extrapolate']=pd.Series.from_csv('cv:extrapolate.csv')\n",
    "train['extrapolate']=pd.Series.from_csv('train:extrapolate.csv')\n",
    "test['extrapolate']=pd.Series.from_csv('test:extrapolate.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cv['extrapolate2'], train['extrapolate2'], test['extrapolate2'] = prebuild_feature_with_date(['group_1'], 'date_y', renamed_feature='extrapolate')\n",
    "#cv['extrapolate2'].to_csv('cv:extrapolate2.csv')\n",
    "#train['extrapolate2'].to_csv('train:extrapolate2.csv')\n",
    "#test['extrapolate2'].to_csv('test:extrapolate2.csv')\n",
    "cv['extrapolate2']=pd.Series.from_csv('cv:extrapolate2.csv')\n",
    "train['extrapolate2']=pd.Series.from_csv('train:extrapolate2.csv')\n",
    "test['extrapolate2']=pd.Series.from_csv('test:extrapolate2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['extrapolate']=cv['extrapolate']\n",
    "train['extrapolate2']=cv['extrapolate2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27593, 0.16515472916422996, 0.42693889885317876)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## what is interesting!\n",
    "def basic_statistics(data):\n",
    "    return sum(data[data['outcome']!=data['extrapolate2']]['extrapolate2']!=-999), \\\n",
    "           sum(np.array(data['extrapolate2']==-999))/len(data), \\\n",
    "           sum(np.array(data['extrapolate']==-999))/len(data)\n",
    "basic_statistics(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1385899371750216, 0.41448443612927549)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(test['extrapolate2']==-999))/len(test), \\\n",
    "sum(np.array(test['extrapolate']==-999))/len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_1_x: 0.0227136510419\n",
      "group_1: 0.631496805867\n",
      "char_2_x: 0.41449417444\n",
      "date_x: 0.103450987051\n",
      "char_3_x: 0.0276144299394\n",
      "char_4_x: 0.0199715515455\n",
      "char_5_x: 0.0188986677505\n",
      "char_6_x: 0.0661623261049\n",
      "char_7_x: 0.106231183299\n",
      "char_8_x: 0.111716684544\n",
      "char_9_x: 0.0998854108276\n",
      "char_10_x: 0.0446959766188\n",
      "char_11: 0.0319390700809\n",
      "char_12: 0.0308145170995\n",
      "char_13: 0.0651726132133\n",
      "char_14: 0.0374490239961\n",
      "char_15: 0.0437610770119\n",
      "char_16: 0.0488039723483\n",
      "char_17: 0.0518955318547\n",
      "char_18: 0.0263508864808\n",
      "char_19: 0.0494459694742\n",
      "char_20: 0.0467037851608\n",
      "char_21: 0.0489115301347\n",
      "char_22: 0.0507054532148\n",
      "char_23: 0.0472229083121\n",
      "char_24: 0.0301810598637\n",
      "char_25: 0.0525013414306\n",
      "char_26: 0.0181899583187\n",
      "char_27: 0.0340701607585\n",
      "char_28: 0.0483006435597\n",
      "char_29: 0.021235698689\n",
      "char_30: 0.026307188195\n",
      "char_31: 0.0395171475062\n",
      "char_32: 0.0485386936629\n",
      "char_33: 0.0272914856664\n",
      "char_34: 0.0625867876657\n",
      "char_35: 0.0279334970705\n",
      "char_36: 0.0634833632596\n",
      "char_37: 0.0524818163032\n",
      "year_x: 0.0037851403231\n",
      "month_x: 0.0137205877349\n",
      "day_x: 0.0189600968306\n",
      "weekday_x: 0.0118898370611\n",
      "date_y: 0.0363289205998\n",
      "activity_category: 0.0178565552447\n",
      "char_1_y: 0.00131840595707\n",
      "char_2_y: 0.00124061330043\n",
      "char_3_y: 0.000666814851958\n",
      "char_4_y: 0.000734561685465\n",
      "char_5_y: 0.00105884696742\n",
      "char_6_y: 0.000602452005257\n",
      "char_7_y: 0.000505338791484\n",
      "char_8_y: 0.000995271339375\n",
      "char_9_y: 0.00101235853705\n",
      "char_10_y: 0.0879476595241\n",
      "outcome: 0.689404562969\n",
      "year_y: 0.00101984058773\n",
      "month_y: 0.00463625050219\n",
      "day_y: 0.00811598903411\n",
      "weekday_y: 0.00109504344107\n"
     ]
    }
   ],
   "source": [
    "a=dict()\n",
    "b=dict()\n",
    "gain=dict()\n",
    "newtrain=cv[cv['extrapolate2']!=-999]\n",
    "for name in newtrain.columns.values:\n",
    "    if name not in set(['people_id', 'activity_id', 'char_38', 'fold', 'date', 'extrapolate', 'extrapolate2']):\n",
    "        print(name, end=': ')\n",
    "        gain[name] = mutual_entropy(newtrain[name], newtrain['outcome'])\n",
    "        print(gain[name])\n",
    "        a[name], b[name] = get_log_posterior(newtrain, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "field_pairs=set()\n",
    "for name, name2 in itertools.combinations(newtrain.columns.values, 2):\n",
    "    non_categorical_fields=set(['people_id', 'activity_id', 'char_38', 'date_x', 'date_y', 'outcome', 'extrapolate', 'extrapolate2'])\n",
    "    if name not in non_categorical_fields and name2 not in non_categorical_fields:\n",
    "        ie, lb = iteraction_entropy(newtrain[name], newtrain[name2], newtrain['outcome'])\n",
    "        if ie > 0.01:\n",
    "            print(name, name2, end=': ')\n",
    "            print(ie)\n",
    "            field_pairs.add((name, name2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train['extrapolate3'], test['extrapolate3'] = prebuild_feature_with_date(['char_3_x'], 'date_x')\n",
    "#train['extrapolate3'].to_csv('train:extrapolate3.csv')\n",
    "#test['extrapolate3'].to_csv('test:extrapolate3.csv')\n",
    "train['extrapolate3']=pd.Series.from_csv('train:extrapolate3.csv')\n",
    "test['extrapolate3']=pd.Series.from_csv('test:extrapolate3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train['extrapolate4'], test['extrapolate4'] = prebuild_feature(['char_7_x'])\n",
    "#train['extrapolate4'].to_csv('train:extrapolate4.csv')\n",
    "#test['extrapolate4'].to_csv('test:extrapolate4.csv')\n",
    "train['extrapolate4']=pd.Series.from_csv('train:extrapolate4.csv')\n",
    "test['extrapolate4']=pd.Series.from_csv('test:extrapolate4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['extrapolate5'], test['extrapolate5'] = prebuild_feature(['char_10_y'])\n",
    "train['extrapolate5'].to_csv('train:extrapolate5.csv')\n",
    "test['extrapolate5'].to_csv('test:extrapolate5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1\n",
      "char_2_x\n",
      "char_6_x\n",
      "char_7_x\n",
      "char_8_x\n",
      "char_9_x\n",
      "char_13\n",
      "char_17\n",
      "char_22\n",
      "char_25\n",
      "char_34\n",
      "char_36\n",
      "char_37\n",
      "char_38\n",
      "char_10_y\n",
      "year_y\n",
      "month_y\n",
      "extrapolate2\n",
      "group_1\n",
      "char_2_x\n",
      "char_6_x\n",
      "char_7_x\n",
      "char_8_x\n",
      "char_9_x\n",
      "char_13\n",
      "char_17\n",
      "char_22\n",
      "char_25\n",
      "char_34\n",
      "char_36\n",
      "char_37\n",
      "char_38\n",
      "char_10_y\n",
      "year_y\n",
      "month_y\n",
      "extrapolate2\n"
     ]
    }
   ],
   "source": [
    "def create_train_data(train, test=None, c=None):\n",
    "    train_feats=np.empty([len(train), 0])\n",
    "    train_label=np.array(train['outcome'])\n",
    "    feature_names=[];\n",
    "    if test is not None:\n",
    "        test_feats=np.empty([len(test), 0])\n",
    "        \n",
    "    for name in train.columns.values:\n",
    "        if name not in set(['people_id', 'activity_id', 'fold', 'char_38', 'outcome', 'date_x', 'date_y']) and \\\n",
    "                            not str(name).startswith('extrapolate') and \\\n",
    "                            (gain[name] > 0.05 or name in set(['year_y','month_y'])):                        \n",
    "            print(name, end=', ')\n",
    "            feature_names=feature_names + [name]\n",
    "            new_feat=np.array([c[name][train[['people_id', name]].to_records(index=False)]]).T            \n",
    "            train_feats=np.append(train_feats, new_feat, 1)\n",
    "            if test is not None:\n",
    "                new_feat=np.array([c[name][test[['people_id', name]].to_records(index=False)]]).T          \n",
    "                test_feats=np.append(test_feats, new_feat, 1)\n",
    "        if name in set(['char_38', 'extrapolate2']):\n",
    "            print(name, end=', ')\n",
    "            feature_names=feature_names + [name]\n",
    "            new_feat=np.array([train[name]], dtype=np.float).T\n",
    "            train_feats=np.append(train_feats, new_feat, 1)\n",
    "            if test is not None:\n",
    "                new_feat=np.array([test[name]], dtype=np.float).T\n",
    "                test_feats=np.append(test_feats, new_feat, 1)\n",
    "    print(' ... [Done]')\n",
    "    if test is None:\n",
    "        return feature_names,train_feats,train_label\n",
    "    else:\n",
    "        return feature_names,train_feats,train_label,test_feats\n",
    "    \n",
    "feature_names,cv_feats,cv_label = create_train_data(cv, None, c)\n",
    "feature_names,train_feats,train_label,test_feats = create_train_data(train, test, c_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group_1, char_2_x, char_6_x, char_7_x, char_8_x, char_9_x, char_13, char_17, char_22, char_25, char_34, char_36, char_37, char_38, char_10_y, year_y, month_y, ... [Done]\n",
      "group_1, char_2_x, char_6_x, char_7_x, char_8_x, char_9_x, char_13, char_17, char_22, char_25, char_34, char_36, char_37, char_38, char_10_y, year_y, month_y, ... [Done]\n"
     ]
    }
   ],
   "source": [
    "def create_train_data2(train, test=None, c=None):\n",
    "    train_feats=np.empty([len(train), 0])\n",
    "    train_label=np.array(train['outcome'])\n",
    "    feature_names=[];\n",
    "    if test is not None:\n",
    "        test_feats=np.empty([len(test), 0])\n",
    "        \n",
    "    for name in train.columns.values:\n",
    "        if name not in set(['people_id', 'activity_id', 'fold', 'char_38', 'outcome', 'date_x', 'date_y']) and \\\n",
    "                            not str(name).startswith('extrapolate') and \\\n",
    "                            (gain[name] > 0.05 or name in set(['year_y','month_y'])):                        \n",
    "            print(name, end=', ')\n",
    "            feature_names=feature_names + [name]\n",
    "            new_feat=np.array([c[name][train[['people_id', name]].to_records(index=False)]]).T            \n",
    "            train_feats=np.append(train_feats, new_feat, 1)\n",
    "            if test is not None:\n",
    "                new_feat=np.array([c[name][test[['people_id', name]].to_records(index=False)]]).T          \n",
    "                test_feats=np.append(test_feats, new_feat, 1)\n",
    "        if name in set(['char_38']):\n",
    "            print(name, end=', ')\n",
    "            feature_names=feature_names + [name]\n",
    "            new_feat=np.array([train[name]], dtype=np.float).T\n",
    "            train_feats=np.append(train_feats, new_feat, 1)\n",
    "            if test is not None:\n",
    "                new_feat=np.array([test[name]], dtype=np.float).T\n",
    "                test_feats=np.append(test_feats, new_feat, 1)\n",
    "    print('... [Done]')\n",
    "    if test is None:\n",
    "        return feature_names,train_feats,train_label\n",
    "    else:\n",
    "        return feature_names,train_feats,train_label,test_feats  \n",
    "\n",
    "feature_names2,cv2_feats,cv2_label = create_train_data2(cv2, None, c)\n",
    "feature_names2,train2_feats,train2_label,test2_feats = create_train_data2(train, test, c_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eta = 0.05\n",
    "gamma = 100\n",
    "max_depth = 3\n",
    "subsample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "\n",
    "params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": eta,\n",
    "        \"gamma\":gamma,\n",
    "        \"max_depth\": max_depth,\n",
    "#        \"subsample\": subsample,\n",
    "#        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        'nthread': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.cross_validation import LabelKFold\n",
    "\n",
    "\n",
    "filter=np.array(cv['extrapolate']==-999) & np.array(cv['extrapolate2']!=-999)\n",
    "fold=LabelKFold(np.array(cv['fold'])[filter], n_folds=5)\n",
    "#fold=LabelKFold(np.array(cv['fold']), n_folds=5)\n",
    "\n",
    "\n",
    "#weight=np.array(1./cv['people_id'].value_counts()[cv['people_id']])\n",
    "dcv=xgb.DMatrix(cv_feats[filter], \n",
    "                cv_label[filter], missing=-999)\n",
    "\n",
    "#alldtrain=xgb.DMatrix(train_feats, \n",
    "#                      train_label, missing=-999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.974301\ttest-auc:0.972834\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 10 rounds.\n",
      "[1]\ttrain-auc:0.975478\ttest-auc:0.973722\n",
      "[2]\ttrain-auc:0.976672\ttest-auc:0.974957\n",
      "[3]\ttrain-auc:0.977741\ttest-auc:0.976046\n",
      "[4]\ttrain-auc:0.977952\ttest-auc:0.975946\n",
      "[5]\ttrain-auc:0.978092\ttest-auc:0.976088\n",
      "[6]\ttrain-auc:0.978273\ttest-auc:0.976081\n",
      "[7]\ttrain-auc:0.978345\ttest-auc:0.976225\n",
      "[8]\ttrain-auc:0.978438\ttest-auc:0.976069\n",
      "[9]\ttrain-auc:0.978443\ttest-auc:0.976249\n",
      "[10]\ttrain-auc:0.978587\ttest-auc:0.976357\n",
      "[11]\ttrain-auc:0.978608\ttest-auc:0.976406\n",
      "[12]\ttrain-auc:0.97893\ttest-auc:0.97666\n",
      "[13]\ttrain-auc:0.979162\ttest-auc:0.97666\n",
      "[14]\ttrain-auc:0.979189\ttest-auc:0.976642\n",
      "[15]\ttrain-auc:0.979185\ttest-auc:0.97665\n",
      "[16]\ttrain-auc:0.980184\ttest-auc:0.978106\n",
      "[17]\ttrain-auc:0.980235\ttest-auc:0.978124\n",
      "[18]\ttrain-auc:0.980324\ttest-auc:0.978151\n",
      "[19]\ttrain-auc:0.980376\ttest-auc:0.978173\n",
      "[20]\ttrain-auc:0.980389\ttest-auc:0.978195\n",
      "[21]\ttrain-auc:0.980999\ttest-auc:0.978481\n",
      "[22]\ttrain-auc:0.981061\ttest-auc:0.978571\n",
      "[23]\ttrain-auc:0.981066\ttest-auc:0.978566\n",
      "[24]\ttrain-auc:0.981084\ttest-auc:0.978605\n",
      "[25]\ttrain-auc:0.982294\ttest-auc:0.979921\n",
      "[26]\ttrain-auc:0.982821\ttest-auc:0.980279\n",
      "[27]\ttrain-auc:0.982876\ttest-auc:0.980306\n",
      "[28]\ttrain-auc:0.982884\ttest-auc:0.980311\n",
      "[29]\ttrain-auc:0.982912\ttest-auc:0.980332\n",
      "[30]\ttrain-auc:0.982989\ttest-auc:0.980421\n",
      "[31]\ttrain-auc:0.983021\ttest-auc:0.980392\n",
      "[32]\ttrain-auc:0.983022\ttest-auc:0.980387\n",
      "[33]\ttrain-auc:0.983128\ttest-auc:0.980398\n",
      "[34]\ttrain-auc:0.983169\ttest-auc:0.980436\n",
      "[35]\ttrain-auc:0.983189\ttest-auc:0.98043\n",
      "[36]\ttrain-auc:0.983185\ttest-auc:0.980415\n",
      "[37]\ttrain-auc:0.983253\ttest-auc:0.980404\n",
      "[38]\ttrain-auc:0.983263\ttest-auc:0.980415\n",
      "[39]\ttrain-auc:0.983293\ttest-auc:0.980361\n",
      "[40]\ttrain-auc:0.983577\ttest-auc:0.980583\n",
      "[41]\ttrain-auc:0.983593\ttest-auc:0.980571\n",
      "[42]\ttrain-auc:0.983601\ttest-auc:0.980546\n",
      "[43]\ttrain-auc:0.983637\ttest-auc:0.980643\n",
      "[44]\ttrain-auc:0.983737\ttest-auc:0.98079\n",
      "[45]\ttrain-auc:0.983847\ttest-auc:0.980779\n",
      "[46]\ttrain-auc:0.983807\ttest-auc:0.980732\n",
      "[47]\ttrain-auc:0.98392\ttest-auc:0.980756\n",
      "[48]\ttrain-auc:0.983935\ttest-auc:0.980725\n",
      "[49]\ttrain-auc:0.983994\ttest-auc:0.980791\n",
      "[50]\ttrain-auc:0.983996\ttest-auc:0.980824\n",
      "[51]\ttrain-auc:0.983986\ttest-auc:0.98087\n",
      "[52]\ttrain-auc:0.984031\ttest-auc:0.980807\n",
      "[53]\ttrain-auc:0.984086\ttest-auc:0.980974\n",
      "[54]\ttrain-auc:0.984127\ttest-auc:0.98098\n",
      "[55]\ttrain-auc:0.98421\ttest-auc:0.981069\n",
      "[56]\ttrain-auc:0.984275\ttest-auc:0.981135\n",
      "[57]\ttrain-auc:0.984352\ttest-auc:0.981257\n",
      "[58]\ttrain-auc:0.984461\ttest-auc:0.981297\n",
      "[59]\ttrain-auc:0.984533\ttest-auc:0.981313\n",
      "[60]\ttrain-auc:0.984617\ttest-auc:0.981308\n",
      "[61]\ttrain-auc:0.984632\ttest-auc:0.981313\n",
      "[62]\ttrain-auc:0.984661\ttest-auc:0.981321\n",
      "[63]\ttrain-auc:0.984672\ttest-auc:0.98136\n",
      "[64]\ttrain-auc:0.984729\ttest-auc:0.981371\n",
      "[65]\ttrain-auc:0.984758\ttest-auc:0.981328\n",
      "[66]\ttrain-auc:0.984809\ttest-auc:0.981333\n",
      "[67]\ttrain-auc:0.984809\ttest-auc:0.981341\n",
      "[68]\ttrain-auc:0.984852\ttest-auc:0.981401\n",
      "[69]\ttrain-auc:0.98491\ttest-auc:0.981458\n",
      "[70]\ttrain-auc:0.984913\ttest-auc:0.981429\n",
      "[71]\ttrain-auc:0.984951\ttest-auc:0.981411\n",
      "[72]\ttrain-auc:0.985033\ttest-auc:0.981647\n",
      "[73]\ttrain-auc:0.98506\ttest-auc:0.981635\n",
      "[74]\ttrain-auc:0.985091\ttest-auc:0.981604\n",
      "[75]\ttrain-auc:0.985093\ttest-auc:0.981617\n",
      "[76]\ttrain-auc:0.9851\ttest-auc:0.981625\n",
      "[77]\ttrain-auc:0.985159\ttest-auc:0.981683\n",
      "[78]\ttrain-auc:0.985204\ttest-auc:0.981717\n",
      "[79]\ttrain-auc:0.985253\ttest-auc:0.981747\n",
      "[80]\ttrain-auc:0.985307\ttest-auc:0.981854\n",
      "[81]\ttrain-auc:0.985337\ttest-auc:0.981862\n",
      "[82]\ttrain-auc:0.985404\ttest-auc:0.981839\n",
      "[83]\ttrain-auc:0.985455\ttest-auc:0.981872\n",
      "[84]\ttrain-auc:0.985477\ttest-auc:0.981856\n",
      "[85]\ttrain-auc:0.98551\ttest-auc:0.98186\n",
      "[86]\ttrain-auc:0.985536\ttest-auc:0.981893\n",
      "[87]\ttrain-auc:0.985552\ttest-auc:0.981908\n",
      "[88]\ttrain-auc:0.985575\ttest-auc:0.981947\n",
      "[89]\ttrain-auc:0.985582\ttest-auc:0.981946\n",
      "[90]\ttrain-auc:0.985605\ttest-auc:0.982011\n",
      "[91]\ttrain-auc:0.985663\ttest-auc:0.981982\n",
      "[92]\ttrain-auc:0.985693\ttest-auc:0.981966\n",
      "[93]\ttrain-auc:0.985715\ttest-auc:0.981983\n",
      "[94]\ttrain-auc:0.985755\ttest-auc:0.981987\n",
      "[95]\ttrain-auc:0.985792\ttest-auc:0.981985\n",
      "[96]\ttrain-auc:0.985845\ttest-auc:0.981977\n",
      "[97]\ttrain-auc:0.985871\ttest-auc:0.982018\n",
      "[98]\ttrain-auc:0.985907\ttest-auc:0.982013\n",
      "[99]\ttrain-auc:0.985969\ttest-auc:0.982068\n",
      "[100]\ttrain-auc:0.98604\ttest-auc:0.982068\n",
      "[101]\ttrain-auc:0.986087\ttest-auc:0.982084\n",
      "[102]\ttrain-auc:0.986137\ttest-auc:0.982078\n",
      "[103]\ttrain-auc:0.98619\ttest-auc:0.982084\n",
      "[104]\ttrain-auc:0.986244\ttest-auc:0.982048\n",
      "[105]\ttrain-auc:0.986271\ttest-auc:0.982081\n",
      "[106]\ttrain-auc:0.986302\ttest-auc:0.982095\n",
      "[107]\ttrain-auc:0.986346\ttest-auc:0.982096\n",
      "[108]\ttrain-auc:0.986395\ttest-auc:0.982095\n",
      "[109]\ttrain-auc:0.986436\ttest-auc:0.982094\n",
      "[110]\ttrain-auc:0.986487\ttest-auc:0.98214\n",
      "[111]\ttrain-auc:0.986554\ttest-auc:0.982147\n",
      "[112]\ttrain-auc:0.986601\ttest-auc:0.982143\n",
      "[113]\ttrain-auc:0.986635\ttest-auc:0.9821\n",
      "[114]\ttrain-auc:0.986679\ttest-auc:0.982108\n",
      "[115]\ttrain-auc:0.986708\ttest-auc:0.982105\n",
      "[116]\ttrain-auc:0.986763\ttest-auc:0.982087\n",
      "[117]\ttrain-auc:0.986795\ttest-auc:0.982079\n",
      "[118]\ttrain-auc:0.986829\ttest-auc:0.982069\n",
      "[119]\ttrain-auc:0.986915\ttest-auc:0.982084\n",
      "[120]\ttrain-auc:0.986979\ttest-auc:0.982082\n",
      "[121]\ttrain-auc:0.987001\ttest-auc:0.982077\n",
      "Stopping. Best iteration:\n",
      "[111]\ttrain-auc:0.986554+0.000396074\ttest-auc:0.982147+0.00233673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "#gbm = xgb.train(params, dtrain, num_boost_round=200, evals=watchlist, early_stopping_rounds=3, verbose_eval=True)\n",
    "model=xgb.cv(params, dcv, 200, folds=fold, \n",
    "             callbacks=[xgb.callback.print_evaluation(show_stdv=False),\n",
    "                        xgb.callback.early_stop(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.740176\ttest-auc:0.731262\n",
      "Multiple eval metrics have been passed: 'test-auc' will be used for early stopping.\n",
      "\n",
      "Will train until test-auc hasn't improved in 10 rounds.\n",
      "[1]\ttrain-auc:0.744467\ttest-auc:0.736467\n",
      "[2]\ttrain-auc:0.745226\ttest-auc:0.73711\n",
      "[3]\ttrain-auc:0.747581\ttest-auc:0.739703\n",
      "[4]\ttrain-auc:0.748178\ttest-auc:0.740349\n",
      "[5]\ttrain-auc:0.748868\ttest-auc:0.741038\n",
      "[6]\ttrain-auc:0.74943\ttest-auc:0.741701\n",
      "[7]\ttrain-auc:0.749902\ttest-auc:0.741978\n",
      "[8]\ttrain-auc:0.750271\ttest-auc:0.742486\n",
      "[9]\ttrain-auc:0.750632\ttest-auc:0.742856\n",
      "[10]\ttrain-auc:0.750799\ttest-auc:0.742836\n",
      "[11]\ttrain-auc:0.751174\ttest-auc:0.743056\n",
      "[12]\ttrain-auc:0.751943\ttest-auc:0.743478\n",
      "[13]\ttrain-auc:0.752206\ttest-auc:0.743587\n",
      "[14]\ttrain-auc:0.752923\ttest-auc:0.74407\n",
      "[15]\ttrain-auc:0.75338\ttest-auc:0.744218\n",
      "[16]\ttrain-auc:0.753598\ttest-auc:0.744229\n",
      "[17]\ttrain-auc:0.754012\ttest-auc:0.744645\n",
      "[18]\ttrain-auc:0.754448\ttest-auc:0.74498\n",
      "[19]\ttrain-auc:0.754687\ttest-auc:0.745271\n",
      "[20]\ttrain-auc:0.75497\ttest-auc:0.745471\n",
      "[21]\ttrain-auc:0.755236\ttest-auc:0.745798\n",
      "[22]\ttrain-auc:0.75553\ttest-auc:0.746058\n",
      "[23]\ttrain-auc:0.755869\ttest-auc:0.746204\n",
      "[24]\ttrain-auc:0.756026\ttest-auc:0.746351\n",
      "[25]\ttrain-auc:0.75636\ttest-auc:0.746599\n",
      "[26]\ttrain-auc:0.75657\ttest-auc:0.746756\n",
      "[27]\ttrain-auc:0.756855\ttest-auc:0.746938\n",
      "[28]\ttrain-auc:0.757116\ttest-auc:0.747143\n",
      "[29]\ttrain-auc:0.757378\ttest-auc:0.747279\n",
      "[30]\ttrain-auc:0.757609\ttest-auc:0.747563\n",
      "[31]\ttrain-auc:0.757858\ttest-auc:0.747671\n",
      "[32]\ttrain-auc:0.758004\ttest-auc:0.747733\n",
      "[33]\ttrain-auc:0.75817\ttest-auc:0.74789\n",
      "[34]\ttrain-auc:0.758373\ttest-auc:0.748068\n",
      "[35]\ttrain-auc:0.758556\ttest-auc:0.748344\n",
      "[36]\ttrain-auc:0.758905\ttest-auc:0.748577\n",
      "[37]\ttrain-auc:0.759225\ttest-auc:0.748829\n",
      "[38]\ttrain-auc:0.759524\ttest-auc:0.749167\n",
      "[39]\ttrain-auc:0.759872\ttest-auc:0.74941\n",
      "[40]\ttrain-auc:0.760108\ttest-auc:0.749589\n",
      "[41]\ttrain-auc:0.760523\ttest-auc:0.750044\n",
      "[42]\ttrain-auc:0.760849\ttest-auc:0.750397\n",
      "[43]\ttrain-auc:0.761177\ttest-auc:0.750683\n",
      "[44]\ttrain-auc:0.761496\ttest-auc:0.750914\n",
      "[45]\ttrain-auc:0.761786\ttest-auc:0.751115\n",
      "[46]\ttrain-auc:0.76201\ttest-auc:0.751355\n",
      "[47]\ttrain-auc:0.762173\ttest-auc:0.751485\n",
      "[48]\ttrain-auc:0.76235\ttest-auc:0.751544\n",
      "[49]\ttrain-auc:0.762592\ttest-auc:0.751713\n",
      "[50]\ttrain-auc:0.762845\ttest-auc:0.751892\n",
      "[51]\ttrain-auc:0.763039\ttest-auc:0.752026\n",
      "[52]\ttrain-auc:0.763224\ttest-auc:0.752112\n",
      "[53]\ttrain-auc:0.763389\ttest-auc:0.752237\n",
      "[54]\ttrain-auc:0.763636\ttest-auc:0.75239\n",
      "[55]\ttrain-auc:0.763836\ttest-auc:0.752534\n",
      "[56]\ttrain-auc:0.764001\ttest-auc:0.752607\n",
      "[57]\ttrain-auc:0.764096\ttest-auc:0.752699\n",
      "[58]\ttrain-auc:0.764272\ttest-auc:0.752772\n",
      "[59]\ttrain-auc:0.764369\ttest-auc:0.752809\n",
      "[60]\ttrain-auc:0.764527\ttest-auc:0.752879\n",
      "[61]\ttrain-auc:0.764659\ttest-auc:0.752931\n",
      "[62]\ttrain-auc:0.764784\ttest-auc:0.752989\n",
      "[63]\ttrain-auc:0.764897\ttest-auc:0.753079\n",
      "[64]\ttrain-auc:0.765026\ttest-auc:0.753132\n",
      "[65]\ttrain-auc:0.765189\ttest-auc:0.753184\n",
      "[66]\ttrain-auc:0.765292\ttest-auc:0.753244\n",
      "[67]\ttrain-auc:0.76542\ttest-auc:0.753348\n",
      "[68]\ttrain-auc:0.765541\ttest-auc:0.753417\n",
      "[69]\ttrain-auc:0.765681\ttest-auc:0.753496\n",
      "[70]\ttrain-auc:0.765743\ttest-auc:0.753559\n",
      "[71]\ttrain-auc:0.765788\ttest-auc:0.753585\n",
      "[72]\ttrain-auc:0.765839\ttest-auc:0.753613\n",
      "[73]\ttrain-auc:0.765876\ttest-auc:0.753628\n",
      "[74]\ttrain-auc:0.76591\ttest-auc:0.753633\n",
      "[75]\ttrain-auc:0.765934\ttest-auc:0.753654\n",
      "[76]\ttrain-auc:0.765987\ttest-auc:0.753692\n",
      "[77]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[78]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[79]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[80]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[81]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[82]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[83]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[84]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[85]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "[86]\ttrain-auc:0.766006\ttest-auc:0.75369\n",
      "Stopping. Best iteration:\n",
      "[76]\ttrain-auc:0.765987+0.00154881\ttest-auc:0.753692+0.00214376\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eta = 0.05\n",
    "gamma = 10\n",
    "max_depth = 6\n",
    "subsample = 0.8\n",
    "colsample_bytree = 0.8\n",
    "\n",
    "params2 = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\" : \"gbtree\",\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"eta\": eta,\n",
    "        \"gamma\":gamma,\n",
    "        \"max_depth\": max_depth,\n",
    "#        \"subsample\": subsample,\n",
    "#        \"colsample_bytree\": colsample_bytree,\n",
    "        \"silent\": 1,\n",
    "        'nthread': 4\n",
    "}\n",
    "\n",
    "\n",
    "fold2=LabelKFold(np.array(cv2['fold']), n_folds=5)\n",
    "\n",
    "\n",
    "weight2=np.array(1./cv2['group_1'].value_counts()[cv2['group_1']])\n",
    "dcv2=xgb.DMatrix(cv2_feats, \n",
    "                 cv2_label, weight=weight2, missing=-999)\n",
    "\n",
    "model2=xgb.cv(params2, dcv2, 200, folds=fold2, \n",
    "              callbacks=[xgb.callback.print_evaluation(show_stdv=False),\n",
    "                         xgb.callback.early_stop(10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.974603\n",
      "[1]\ttrain-auc:0.97527\n",
      "[2]\ttrain-auc:0.977138\n",
      "[3]\ttrain-auc:0.978153\n",
      "[4]\ttrain-auc:0.978226\n",
      "[5]\ttrain-auc:0.978155\n",
      "[6]\ttrain-auc:0.9783\n",
      "[7]\ttrain-auc:0.978301\n",
      "[8]\ttrain-auc:0.978218\n",
      "[9]\ttrain-auc:0.979233\n",
      "[10]\ttrain-auc:0.979232\n",
      "[11]\ttrain-auc:0.9793\n",
      "[12]\ttrain-auc:0.979289\n",
      "[13]\ttrain-auc:0.979373\n",
      "[14]\ttrain-auc:0.979349\n",
      "[15]\ttrain-auc:0.979344\n",
      "[16]\ttrain-auc:0.979424\n",
      "[17]\ttrain-auc:0.979349\n",
      "[18]\ttrain-auc:0.979378\n",
      "[19]\ttrain-auc:0.97939\n",
      "[20]\ttrain-auc:0.981722\n",
      "[21]\ttrain-auc:0.981722\n",
      "[22]\ttrain-auc:0.981723\n",
      "[23]\ttrain-auc:0.981745\n",
      "[24]\ttrain-auc:0.981741\n",
      "[25]\ttrain-auc:0.981812\n",
      "[26]\ttrain-auc:0.981852\n",
      "[27]\ttrain-auc:0.981926\n",
      "[28]\ttrain-auc:0.981923\n",
      "[29]\ttrain-auc:0.981937\n",
      "[30]\ttrain-auc:0.981935\n",
      "[31]\ttrain-auc:0.982242\n",
      "[32]\ttrain-auc:0.982794\n",
      "[33]\ttrain-auc:0.982879\n",
      "[34]\ttrain-auc:0.982922\n",
      "[35]\ttrain-auc:0.982936\n",
      "[36]\ttrain-auc:0.983052\n",
      "[37]\ttrain-auc:0.983107\n",
      "[38]\ttrain-auc:0.983063\n",
      "[39]\ttrain-auc:0.983067\n",
      "[40]\ttrain-auc:0.983107\n",
      "[41]\ttrain-auc:0.98311\n",
      "[42]\ttrain-auc:0.983068\n",
      "[43]\ttrain-auc:0.983109\n",
      "[44]\ttrain-auc:0.983105\n",
      "[45]\ttrain-auc:0.983064\n",
      "[46]\ttrain-auc:0.983105\n",
      "[47]\ttrain-auc:0.983088\n",
      "[48]\ttrain-auc:0.983097\n",
      "[49]\ttrain-auc:0.983096\n",
      "[50]\ttrain-auc:0.983104\n",
      "[51]\ttrain-auc:0.983096\n",
      "[52]\ttrain-auc:0.983054\n",
      "[53]\ttrain-auc:0.983095\n",
      "[54]\ttrain-auc:0.983099\n",
      "[55]\ttrain-auc:0.983097\n",
      "[56]\ttrain-auc:0.983178\n",
      "[57]\ttrain-auc:0.983327\n",
      "[58]\ttrain-auc:0.983507\n",
      "[59]\ttrain-auc:0.983639\n",
      "[60]\ttrain-auc:0.983616\n",
      "[61]\ttrain-auc:0.983642\n",
      "[62]\ttrain-auc:0.983682\n",
      "[63]\ttrain-auc:0.98384\n",
      "[64]\ttrain-auc:0.983836\n",
      "[65]\ttrain-auc:0.984001\n",
      "[66]\ttrain-auc:0.984353\n",
      "[67]\ttrain-auc:0.984386\n",
      "[68]\ttrain-auc:0.984439\n",
      "[69]\ttrain-auc:0.984509\n",
      "[70]\ttrain-auc:0.98452\n",
      "[71]\ttrain-auc:0.984542\n",
      "[72]\ttrain-auc:0.984538\n",
      "[73]\ttrain-auc:0.984616\n",
      "[74]\ttrain-auc:0.984595\n",
      "[75]\ttrain-auc:0.984583\n",
      "[76]\ttrain-auc:0.984637\n",
      "[77]\ttrain-auc:0.9847\n",
      "[78]\ttrain-auc:0.984686\n",
      "[79]\ttrain-auc:0.984701\n",
      "[80]\ttrain-auc:0.984709\n",
      "[81]\ttrain-auc:0.984791\n",
      "[82]\ttrain-auc:0.984765\n",
      "[83]\ttrain-auc:0.984809\n",
      "[84]\ttrain-auc:0.985005\n",
      "[85]\ttrain-auc:0.985104\n",
      "[86]\ttrain-auc:0.985147\n",
      "[87]\ttrain-auc:0.98515\n",
      "[88]\ttrain-auc:0.985188\n",
      "[89]\ttrain-auc:0.985213\n",
      "[90]\ttrain-auc:0.985233\n",
      "[91]\ttrain-auc:0.985236\n",
      "[92]\ttrain-auc:0.985304\n",
      "[93]\ttrain-auc:0.985342\n",
      "[94]\ttrain-auc:0.985327\n",
      "[95]\ttrain-auc:0.985409\n",
      "[96]\ttrain-auc:0.98547\n",
      "[97]\ttrain-auc:0.985471\n",
      "[98]\ttrain-auc:0.985552\n",
      "[99]\ttrain-auc:0.985682\n",
      "[100]\ttrain-auc:0.98568\n",
      "[101]\ttrain-auc:0.985684\n",
      "[102]\ttrain-auc:0.985731\n",
      "[103]\ttrain-auc:0.985764\n",
      "[104]\ttrain-auc:0.985801\n",
      "[105]\ttrain-auc:0.985816\n",
      "[106]\ttrain-auc:0.985822\n",
      "[107]\ttrain-auc:0.985867\n",
      "[108]\ttrain-auc:0.986062\n",
      "[109]\ttrain-auc:0.986079\n",
      "[110]\ttrain-auc:0.986091\n",
      "{'f4': 8, 'f9': 47, 'f12': 3, 'f10': 2, 'f6': 17, 'f0': 246, 'f5': 12, 'f16': 5, 'f7': 22, 'f13': 4, 'f3': 60, 'f15': 5, 'f11': 12, 'f17': 210, 'f1': 10, 'f2': 28}\n"
     ]
    }
   ],
   "source": [
    "filter=np.array(train['extrapolate']==-999) & np.array(train['extrapolate2']!=-999)\n",
    "dtrain=xgb.DMatrix(train_feats[filter],\n",
    "                   train_label[filter], missing=-999)\n",
    "\n",
    "filter_test=np.array(test['extrapolate']==-999) & np.array(test['extrapolate2']!=-999)\n",
    "dtest=xgb.DMatrix(test_feats[filter_test], missing=-999)\n",
    "\n",
    "gbm1 = xgb.train(params, dtrain, num_boost_round=111, evals=[(dtrain, 'train')], verbose_eval=True)\n",
    "print(gbm1.get_fscore())\n",
    "\n",
    "submission1=pd.DataFrame({'activity_id':test['activity_id'][filter_test],'outcome': gbm1.predict(dtest)})\n",
    "submission0=pd.DataFrame({'activity_id':test['activity_id'][test['extrapolate']!=-999],'outcome': test['extrapolate'][test['extrapolate']!=-999]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-auc:0.745994\n",
      "[1]\ttrain-auc:0.750388\n",
      "[2]\ttrain-auc:0.752023\n",
      "[3]\ttrain-auc:0.751655\n",
      "[4]\ttrain-auc:0.75302\n",
      "[5]\ttrain-auc:0.753209\n",
      "[6]\ttrain-auc:0.753737\n",
      "[7]\ttrain-auc:0.754086\n",
      "[8]\ttrain-auc:0.754474\n",
      "[9]\ttrain-auc:0.754739\n",
      "[10]\ttrain-auc:0.754674\n",
      "[11]\ttrain-auc:0.755266\n",
      "[12]\ttrain-auc:0.755592\n",
      "[13]\ttrain-auc:0.755638\n",
      "[14]\ttrain-auc:0.756121\n",
      "[15]\ttrain-auc:0.757045\n",
      "[16]\ttrain-auc:0.757373\n",
      "[17]\ttrain-auc:0.757843\n",
      "[18]\ttrain-auc:0.757836\n",
      "[19]\ttrain-auc:0.758538\n",
      "[20]\ttrain-auc:0.759144\n",
      "[21]\ttrain-auc:0.759503\n",
      "[22]\ttrain-auc:0.759788\n",
      "[23]\ttrain-auc:0.760062\n"
     ]
    }
   ],
   "source": [
    "weight2=np.array(1./train['group_1'].value_counts()[train['group_1']])\n",
    "dtrain2=xgb.DMatrix(train2_feats,\n",
    "                    train2_label, weight=weight2, missing=-999)\n",
    "\n",
    "filter_test2=np.array(test['extrapolate2']==-999)\n",
    "dtest2=xgb.DMatrix(test2_feats[filter_test2], missing=-999)\n",
    "\n",
    "gbm2 = xgb.train(params2, dtrain2, num_boost_round=77, evals=[(dtrain2, 'train')], verbose_eval=True)\n",
    "print(gbm2.get_fscore())\n",
    "\n",
    "submission2=pd.DataFrame({'activity_id':test['activity_id'][filter_test2],'outcome': gbm2.predict(dtest2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission=pd.concat((submission0, submission1, submission2))\n",
    "submission.to_csv('submit_xgb_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
